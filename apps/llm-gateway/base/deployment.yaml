apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-gateway
  namespace: llm-gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-gateway
  template:
    metadata:
      labels:
        app: llm-gateway
    spec:
      containers:
        - name: llm-gateway
          image: python:3.11-slim
          command: ["/bin/bash"]
          args:
            - -c
            - |
              pip install fastapi uvicorn httpx pydantic
              cat > /app/main.py << 'EOF'
              from fastapi import FastAPI, HTTPException
              from pydantic import BaseModel
              import httpx
              import os
              import logging

              # Configure logging
              logging.basicConfig(level=logging.INFO)
              logger = logging.getLogger(__name__)

              app = FastAPI(title="LLM Gateway", version="1.0.0")

              OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama-llama3.ollama.svc.cluster.local:11434")
              DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "llama3")

              class ChatRequest(BaseModel):
                  message: str
                  model: str = DEFAULT_MODEL
                  context: str = ""
                  temperature: float = 0.7
                  max_tokens: int = 1000
                  task_type: str = "general"

              class ChatResponse(BaseModel):
                  response: str
                  model: str
                  usage: dict

              def format_prompt_for_task(message: str, context: str, task_type: str) -> str:
                  if task_type == "security_analysis":
                      return f"""You are a cybersecurity expert analyzing potential threats.

              Context: {context}
              Task: Analyze the following for security threats and provide:
              1. Risk Level: [LOW/MEDIUM/HIGH]
              2. Threat Type: [if applicable]
              3. Analysis: [detailed explanation]
              4. Recommendations: [actionable steps]

              Data to analyze: {message}

              Analysis:"""
                  elif task_type == "code_analysis":
                      return f"""You are a security-focused code reviewer.

              Context: {context}
              Task: Review the following code for security vulnerabilities:

              {message}

              Review:"""
                  else:
                      return f"{context}\n\nUser: {message}\nAssistant:"

              @app.post("/chat", response_model=ChatResponse)
              async def chat(request: ChatRequest):
                  try:
                      logger.info(f"LLM request: model={request.model}, task_type={request.task_type}")
                      
                      formatted_prompt = format_prompt_for_task(
                          request.message, 
                          request.context, 
                          request.task_type
                      )
                      
                      # Adjust temperature based on task type
                      temp = 0.2 if request.task_type in ["security_analysis", "code_analysis"] else request.temperature
                      
                      async with httpx.AsyncClient(timeout=300.0) as client:
                          ollama_request = {
                              "model": request.model,
                              "prompt": formatted_prompt,
                              "stream": False,
                              "options": {
                                  "temperature": temp,
                                  "num_predict": request.max_tokens
                              }
                          }
                          
                          response = await client.post(
                              f"{OLLAMA_BASE_URL}/api/generate",
                              json=ollama_request
                          )
                          response.raise_for_status()
                          
                          result = response.json()
                          
                          return ChatResponse(
                              response=result.get("response", ""),
                              model=request.model,
                              usage={
                                  "prompt_tokens": result.get("prompt_eval_count", 0),
                                  "completion_tokens": result.get("eval_count", 0),
                                  "total_time": result.get("total_duration", 0)
                              }
                          )
                          
                  except Exception as e:
                      logger.error(f"LLM request failed: {str(e)}")
                      raise HTTPException(status_code=500, detail=str(e))

              @app.get("/models")
              async def list_models():
                  try:
                      async with httpx.AsyncClient() as client:
                          response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
                          return response.json()
                  except Exception as e:
                      raise HTTPException(status_code=500, detail=str(e))

              @app.get("/health")
              async def health_check():
                  return {"status": "healthy"}

              @app.get("/ready")
              async def readiness_check():
                  try:
                      async with httpx.AsyncClient(timeout=5.0) as client:
                          await client.get(f"{OLLAMA_BASE_URL}/api/tags")
                      return {"status": "ready"}
                  except:
                      raise HTTPException(status_code=503, detail="Ollama not available")
              EOF
              cd /app && uvicorn main:app --host 0.0.0.0 --port 8080
          ports:
            - containerPort: 8080
          env:
            - name: OLLAMA_BASE_URL
              value: "http://ollama-llama3.ollama.svc.cluster.local:11434"
            - name: DEFAULT_MODEL
              value: "llama3"
            - name: LOG_LEVEL
              value: "INFO"
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
