apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-gateway
  namespace: llm-gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-gateway
  template:
    metadata:
      labels:
        app: llm-gateway
    spec:
      containers:
        - name: llm-gateway
          image: python:3.11-slim
          command: ["/bin/bash"]
          args:
            - -c
            - |
              # Install packages without version constraints
              pip install fastapi uvicorn httpx pydantic
              
              # Create the app
              mkdir -p /app
              cp /config/main.py /app/main.py
              
              # Start the application
              cd /app && uvicorn main:app --host 0.0.0.0 --port 8080
          ports:
            - containerPort: 8080
          env:
            - name: OLLAMA_BASE_URL
              value: "http://ollama-llama3.ollama.svc.cluster.local:11434"
            - name: DEFAULT_MODEL
              value: "llama3"
            - name: LOG_LEVEL
              value: "INFO"
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          volumeMounts:
            - name: app-config
              mountPath: /config
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 5
      volumes:
        - name: app-config
          configMap:
            name: llm-gateway-app
