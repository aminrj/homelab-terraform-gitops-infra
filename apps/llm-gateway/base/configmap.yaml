apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-app
  namespace: llm-gateway
data:
  main.py: |
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    import httpx
    import os
    import logging

    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    app = FastAPI(title="LLM Gateway", version="1.0.0")

    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama-llama3.ollama.svc.cluster.local:11434")
    DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "llama3")

    class ChatRequest(BaseModel):
        message: str
        model: str = DEFAULT_MODEL
        context: str = ""
        temperature: float = 0.7
        max_tokens: int = 1000
        task_type: str = "general"

    class ChatResponse(BaseModel):
        response: str
        model: str
        usage: dict

    def format_prompt_for_task(message: str, context: str, task_type: str) -> str:
        if task_type == "security_analysis":
            return f"""You are a cybersecurity expert analyzing potential threats.

    Context: {context}
    Task: Analyze the following for security threats and provide:
    1. Risk Level: [LOW/MEDIUM/HIGH]
    2. Threat Type: [if applicable]
    3. Analysis: [detailed explanation]
    4. Recommendations: [actionable steps]

    Data to analyze: {message}

    Analysis:"""
        elif task_type == "code_analysis":
            return f"""You are a security-focused code reviewer.

    Context: {context}
    Task: Review the following code for security vulnerabilities:

    {message}

    Review:"""
        else:
            return f"{context}\n\nUser: {message}\nAssistant:"

    @app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        try:
            logger.info(f"LLM request: model={request.model}, task_type={request.task_type}")
            
            formatted_prompt = format_prompt_for_task(
                request.message, 
                request.context, 
                request.task_type
            )
            
            # Adjust temperature based on task type
            temp = 0.2 if request.task_type in ["security_analysis", "code_analysis"] else request.temperature
            
            async with httpx.AsyncClient(timeout=300.0) as client:
                ollama_request = {
                    "model": request.model,
                    "prompt": formatted_prompt,
                    "stream": False,
                    "options": {
                        "temperature": temp,
                        "num_predict": request.max_tokens
                    }
                }
                
                response = await client.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json=ollama_request
                )
                response.raise_for_status()
                
                result = response.json()
                
                return ChatResponse(
                    response=result.get("response", ""),
                    model=request.model,
                    usage={
                        "prompt_tokens": result.get("prompt_eval_count", 0),
                        "completion_tokens": result.get("eval_count", 0),
                        "total_time": result.get("total_duration", 0)
                    }
                )
                
        except Exception as e:
            logger.error(f"LLM request failed: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/models")
    async def list_models():
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
                return response.json()
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/health")
    async def health_check():
        return {"status": "healthy"}

    @app.get("/ready")
    async def readiness_check():
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            return {"status": "ready"}
        except:
            raise HTTPException(status_code=503, detail="Ollama not available")

  requirements.txt: |
    fastapi
    uvicorn
    httpx
    pydantic
