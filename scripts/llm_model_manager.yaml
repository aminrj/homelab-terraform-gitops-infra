# Enhanced LLM Model Manager
# This creates multiple Ollama instances with different models for quick testing

---
# Model Manager Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: llm-models
  labels:
    app.kubernetes.io/name: llm-models

---
# Small Models for Quick Testing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-small-models
  namespace: llm-models
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-small
  template:
    metadata:
      labels:
        app: ollama-small
        model-size: small
    spec:
      nodeSelector:
        llm: "true"
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
        env:
        - name: OLLAMA_MODELS
          value: /root/.ollama/models
        - name: OLLAMA_HOST
          value: "0.0.0.0:11434"
        - name: OLLAMA_KEEP_ALIVE
          value: "10m"
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
          limits:
            cpu: "4"
            memory: "8Gi"
        volumeMounts:
        - name: small-model-cache
          mountPath: /root/.ollama
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                sleep 10
                ollama pull phi3:mini
                ollama pull qwen2.5:1.5b
                ollama pull tinyllama
      volumes:
      - name: small-model-cache
        persistentVolumeClaim:
          claimName: small-model-cache

---
# Medium Models 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-medium-models
  namespace: llm-models
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-medium
  template:
    metadata:
      labels:
        app: ollama-medium
        model-size: medium
    spec:
      nodeSelector:
        llm: "true"
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
        env:
        - name: OLLAMA_MODELS
          value: /root/.ollama/models
        - name: OLLAMA_HOST
          value: "0.0.0.0:11434"
        - name: OLLAMA_KEEP_ALIVE
          value: "15m"
        resources:
          requests:
            cpu: "4"
            memory: "12Gi"
          limits:
            cpu: "6"
            memory: "16Gi"
        volumeMounts:
        - name: medium-model-cache
          mountPath: /root/.ollama
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                sleep 15
                ollama pull llama3.2:3b
                ollama pull mistral:7b
                ollama pull qwen2.5:7b
      volumes:
      - name: medium-model-cache
        persistentVolumeClaim:
          claimName: medium-model-cache

---
# Services for Model Access
apiVersion: v1
kind: Service
metadata:
  name: ollama-small-service
  namespace: llm-models
spec:
  selector:
    app: ollama-small
  ports:
  - port: 11434
    targetPort: 11434
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: ollama-medium-service
  namespace: llm-models
spec:
  selector:
    app: ollama-medium
  ports:
  - port: 11434
    targetPort: 11434
  type: ClusterIP

---
# Keep your existing large model service
apiVersion: v1
kind: Service
metadata:
  name: ollama-large-service
  namespace: llm-models
spec:
  type: ExternalName
  externalName: ollama-service.ollama.svc.cluster.local
  ports:
  - port: 11434

---
# PVCs for model caches
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: small-model-cache
  namespace: llm-models
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: medium-model-cache
  namespace: llm-models
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi

---
# Enhanced LLM Gateway with Model Router
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-gateway-enhanced
  namespace: llm-gateway
data:
  main.py: |
    from fastapi import FastAPI, HTTPException
    from fastapi.responses import HTMLResponse
    from pydantic import BaseModel
    import httpx
    import os
    import logging
    import time
    import json
    from typing import Dict, Any, List, Optional

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    app = FastAPI(title="Enhanced LLM Gateway", version="2.0.0")

    # Model routing configuration
    MODEL_ROUTES = {
        # Small models for quick testing
        "phi3:mini": "http://ollama-small-service.llm-models.svc.cluster.local:11434",
        "qwen2.5:1.5b": "http://ollama-small-service.llm-models.svc.cluster.local:11434", 
        "tinyllama": "http://ollama-small-service.llm-models.svc.cluster.local:11434",
        
        # Medium models for balanced performance
        "llama3.2:3b": "http://ollama-medium-service.llm-models.svc.cluster.local:11434",
        "mistral:7b": "http://ollama-medium-service.llm-models.svc.cluster.local:11434",
        "qwen2.5:7b": "http://ollama-medium-service.llm-models.svc.cluster.local:11434",
        
        # Large models for best quality
        "llama3": "http://ollama-service.ollama.svc.cluster.local:11434",
        "llama3:latest": "http://ollama-service.ollama.svc.cluster.local:11434",
    }

    # Default model selections by task type
    TASK_MODEL_DEFAULTS = {
        "quick_test": "phi3:mini",
        "security_analysis": "mistral:7b", 
        "code_analysis": "qwen2.5:7b",
        "incident_response": "llama3.2:3b",
        "threat_intelligence": "llama3",
        "general": "phi3:mini"
    }

    class ChatRequest(BaseModel):
        message: str
        model: Optional[str] = None
        context: str = ""
        temperature: float = 0.7
        max_tokens: int = 1000
        task_type: str = "general"
        auto_select_model: bool = True

    class ChatResponse(BaseModel):
        response: str
        model: str
        backend_url: str
        usage: dict
        processing_time_ms: int
        model_recommendation: Optional[str] = None

    class ModelInfo(BaseModel):
        name: str
        size: str
        backend: str
        recommended_for: List[str]
        avg_response_time: Optional[float] = None

    def get_model_backend(model: str) -> str:
        """Get the backend URL for a specific model"""
        return MODEL_ROUTES.get(model, MODEL_ROUTES["phi3:mini"])

    def auto_select_model(task_type: str, message_length: int) -> str:
        """Automatically select best model for task and message length"""
        if message_length < 100:
            return "phi3:mini"  # Quick responses for short messages
        elif task_type in TASK_MODEL_DEFAULTS:
            return TASK_MODEL_DEFAULTS[task_type]
        else:
            return "llama3.2:3b"  # Balanced default

    @app.get("/", response_class=HTMLResponse)
    async def dashboard():
        return '''<!DOCTYPE html>
<html>
<head>
    <title>🚀 Enhanced LLM Testing Dashboard</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, sans-serif; margin: 0; background: #f5f7fa; }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }
        .card { background: white; border-radius: 12px; padding: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
        .model-selector { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; }
        .model-card { padding: 15px; border: 2px solid #e0e6ed; border-radius: 8px; cursor: pointer; transition: all 0.2s; }
        .model-card:hover { border-color: #667eea; background: #f8f9ff; }
        .model-card.selected { border-color: #667eea; background: #667eea; color: white; }
        textarea { width: 100%; min-height: 120px; border: 2px solid #e0e6ed; border-radius: 8px; padding: 12px; }
        button { background: #667eea; color: white; border: none; padding: 12px 24px; border-radius: 8px; cursor: pointer; font-weight: 600; }
        button:hover { background: #5a6fd8; }
        .response { background: #f8f9fa; border-left: 4px solid #667eea; padding: 20px; margin: 20px 0; border-radius: 8px; }
        .metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px; margin: 15px 0; }
        .metric { text-align: center; padding: 10px; background: rgba(102, 126, 234, 0.1); border-radius: 6px; }
        .quick-tests { display: flex; gap: 10px; flex-wrap: wrap; margin: 15px 0; }
        .quick-test { background: #28a745; color: white; border: none; padding: 8px 16px; border-radius: 6px; cursor: pointer; font-size: 14px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>🚀 Enhanced LLM Testing Dashboard</h1>
        <p>Test multiple models, compare performance, develop workflows with ease</p>
    </div>
    
    <div class="container">
        <div class="grid">
            <div class="card">
                <h3>📝 Test Input</h3>
                <div class="quick-tests">
                    <button class="quick-test" onclick="loadTest('security')">🔒 Security Alert</button>
                    <button class="quick-test" onclick="loadTest('code')">💻 Code Review</button>
                    <button class="quick-test" onclick="loadTest('quick')">⚡ Quick Test</button>
                </div>
                <textarea id="message" placeholder="Enter your test message..."></textarea>
                
                <h4>🎯 Task Type</h4>
                <select id="task_type" onchange="updateModelRecommendation()">
                    <option value="quick_test">⚡ Quick Test</option>
                    <option value="security_analysis">🔒 Security Analysis</option>
                    <option value="code_analysis">💻 Code Analysis</option>
                    <option value="incident_response">🚨 Incident Response</option>
                    <option value="general">💬 General</option>
                </select>
            </div>
            
            <div class="card">
                <h3>🤖 Model Selection</h3>
                <label>
                    <input type="checkbox" id="auto_select" checked onchange="toggleAutoSelect()"> 
                    Auto-select best model for task
                </label>
                <div id="model_recommendation" style="margin: 10px 0; font-style: italic;"></div>
                
                <div class="model-selector" id="models">
                    <div class="model-card" data-model="phi3:mini">
                        <strong>Phi-3 Mini</strong><br>
                        <small>⚡ Fastest • 1.8B params</small>
                    </div>
                    <div class="model-card" data-model="llama3.2:3b">
                        <strong>Llama 3.2 3B</strong><br>
                        <small>⚖️ Balanced • 3B params</small>
                    </div>
                    <div class="model-card" data-model="mistral:7b">
                        <strong>Mistral 7B</strong><br>
                        <small>🎯 Precise • 7B params</small>
                    </div>
                    <div class="model-card" data-model="llama3">
                        <strong>Llama 3 8B</strong><br>
                        <small>🏆 Best Quality • 8B params</small>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="card">
            <button onclick="runTest()" style="width: 100%;">🚀 Run Test</button>
            <div id="loading" style="display: none; text-align: center; margin: 20px 0;">
                Testing models... ⏳
            </div>
            <div id="results"></div>
        </div>
        
        <div class="card">
            <h3>📊 Model Comparison</h3>
            <button onclick="runComparison()">Compare All Models</button>
            <div id="comparison"></div>
        </div>
    </div>

    <script>
        let selectedModel = 'phi3:mini';
        
        const testData = {
            security: "ALERT: Multiple failed SSH login attempts detected from IP 192.168.1.100. 47 attempts in 5 minutes targeting root account.",
            code: "def execute_query(sql, params): return db.execute(sql % params)",
            quick: "Hello, how are you?"
        };
        
        function loadTest(type) {
            document.getElementById('message').value = testData[type];
            if (type === 'security') document.getElementById('task_type').value = 'security_analysis';
            if (type === 'code') document.getElementById('task_type').value = 'code_analysis';
            updateModelRecommendation();
        }
        
        function updateModelRecommendation() {
            const taskType = document.getElementById('task_type').value;
            const recommendations = {
                'quick_test': 'Phi-3 Mini (fastest response)',
                'security_analysis': 'Mistral 7B (security focused)',
                'code_analysis': 'Qwen 2.5 7B (code specialist)',
                'incident_response': 'Llama 3.2 3B (balanced)',
                'general': 'Phi-3 Mini (general use)'
            };
            document.getElementById('model_recommendation').textContent = 
                '💡 Recommended: ' + recommendations[taskType];
        }
        
        function toggleAutoSelect() {
            const auto = document.getElementById('auto_select').checked;
            document.querySelectorAll('.model-card').forEach(card => {
                card.style.opacity = auto ? '0.5' : '1';
                card.style.pointerEvents = auto ? 'none' : 'auto';
            });
        }
        
        document.querySelectorAll('.model-card').forEach(card => {
            card.addEventListener('click', () => {
                document.querySelectorAll('.model-card').forEach(c => c.classList.remove('selected'));
                card.classList.add('selected');
                selectedModel = card.dataset.model;
            });
        });
        
        async function runTest() {
            const message = document.getElementById('message').value;
            const taskType = document.getElementById('task_type').value;
            const autoSelect = document.getElementById('auto_select').checked;
            
            document.getElementById('loading').style.display = 'block';
            document.getElementById('results').innerHTML = '';
            
            try {
                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        message,
                        task_type: taskType,
                        model: autoSelect ? null : selectedModel,
                        auto_select_model: autoSelect
                    })
                });
                
                const data = await response.json();
                displayResult(data);
            } catch (error) {
                document.getElementById('results').innerHTML = `<div style="color: red;">Error: ${error.message}</div>`;
            } finally {
                document.getElementById('loading').style.display = 'none';
            }
        }
        
        function displayResult(data) {
            const html = `
                <div class="response">
                    <div class="metrics">
                        <div class="metric"><strong>Model</strong><br>${data.model}</div>
                        <div class="metric"><strong>Time</strong><br>${data.processing_time_ms}ms</div>
                        <div class="metric"><strong>Tokens/sec</strong><br>${Math.round(data.usage.tokens_per_second || 0)}</div>
                        <div class="metric"><strong>Backend</strong><br>${data.backend_url.split('.')[0].split('//')[1]}</div>
                    </div>
                    <pre style="white-space: pre-wrap; background: white; padding: 15px; border-radius: 6px;">${data.response}</pre>
                </div>
            `;
            document.getElementById('results').innerHTML = html;
        }
        
        async function runComparison() {
            const message = document.getElementById('message').value || "What is artificial intelligence?";
            const models = ['phi3:mini', 'llama3.2:3b', 'mistral:7b'];
            
            document.getElementById('comparison').innerHTML = 'Running comparison across models... ⏳';
            
            const results = [];
            for (const model of models) {
                try {
                    const start = Date.now();
                    const response = await fetch('/chat', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ message, model, auto_select_model: false })
                    });
                    const data = await response.json();
                    results.push({
                        model,
                        time: Date.now() - start,
                        tokens_per_sec: data.usage.tokens_per_second || 0,
                        response_length: data.response.length
                    });
                } catch (error) {
                    results.push({ model, error: error.message });
                }
            }
            
            const comparisonHtml = results.map(r => 
                `<div style="border: 1px solid #ddd; padding: 10px; margin: 5px 0; border-radius: 6px;">
                    <strong>${r.model}</strong> - 
                    ${r.error ? `❌ ${r.error}` : `✅ ${r.time}ms, ${Math.round(r.tokens_per_sec)} tok/s`}
                </div>`
            ).join('');
            
            document.getElementById('comparison').innerHTML = comparisonHtml;
        }
        
        // Initialize
        updateModelRecommendation();
        loadTest('quick');
    </script>
</body>
</html>'''

    @app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        start_time = time.time()
        
        # Auto-select model if requested
        if request.auto_select_model or not request.model:
            selected_model = auto_select_model(request.task_type, len(request.message))
            recommendation = f"Auto-selected {selected_model} for {request.task_type}"
        else:
            selected_model = request.model
            recommendation = None
        
        backend_url = get_model_backend(selected_model)
        
        try:
            # Simple prompt formatting
            if request.task_type == "security_analysis":
                prompt = f"Analyze this security event and provide risk level (HIGH/MEDIUM/LOW), threat type, and recommendations:\n\n{request.message}"
            elif request.task_type == "code_analysis":
                prompt = f"Review this code for security vulnerabilities:\n\n{request.message}"
            elif request.task_type == "quick_test":
                prompt = f"Respond briefly: {request.message}"
            else:
                prompt = request.message
            
            if request.context:
                prompt = f"Context: {request.context}\n\n{prompt}"
            
            async with httpx.AsyncClient(timeout=180.0) as client:
                response = await client.post(
                    f"{backend_url}/api/generate",
                    json={
                        "model": selected_model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": request.temperature,
                            "num_predict": request.max_tokens
                        }
                    }
                )
                response.raise_for_status()
                result = response.json()
                
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                return ChatResponse(
                    response=result.get("response", ""),
                    model=selected_model,
                    backend_url=backend_url,
                    usage={
                        "prompt_tokens": result.get("prompt_eval_count", 0),
                        "completion_tokens": result.get("eval_count", 0),
                        "tokens_per_second": result.get("eval_count", 0) / (result.get("eval_duration", 1) / 1000000000) if result.get("eval_duration") else 0
                    },
                    processing_time_ms=processing_time_ms,
                    model_recommendation=recommendation
                )
                
        except Exception as e:
            logger.error(f"Chat request failed: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/models")
    async def list_models():
        """Get all available models across all backends"""
        all_models = []
        
        for model, backend in MODEL_ROUTES.items():
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(f"{backend}/api/tags")
                    if response.status_code == 200:
                        backend_models = response.json().get("models", [])
                        for m in backend_models:
                            if m["name"] == model:
                                all_models.append({
                                    "name": model,
                                    "backend": backend,
                                    "size": m.get("size", 0),
                                    "modified": m.get("modified_at", "")
                                })
            except:
                # Backend not available
                pass
                
        return {"models": all_models}

    @app.get("/health")
    async def health_check():
        return {"status": "healthy", "timestamp": time.time(), "version": "2.0.0"}

---
# Update your existing LLM Gateway deployment to use the enhanced version
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-gateway-enhanced
  namespace: llm-gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-gateway-enhanced
  template:
    metadata:
      labels:
        app: llm-gateway-enhanced
    spec:
      containers:
      - name: llm-gateway
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install fastapi uvicorn httpx pydantic
          mkdir -p /app
          cp /config/main.py /app/main.py
          cd /app && uvicorn main:app --host 0.0.0.0 --port 8080
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "1Gi"
        volumeMounts:
        - name: app-config
          mountPath: /config
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: app-config
        configMap:
          name: llm-gateway-enhanced

---
apiVersion: v1
kind: Service
metadata:
  name: llm-gateway-enhanced-service
  namespace: llm-gateway
spec:
  selector:
    app: llm-gateway-enhanced
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP

---
# Ingress for easy access without port-forwarding
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-testing-ingress
  namespace: llm-gateway
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: llm-test.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-gateway-enhanced-service
            port:
              number: 80
