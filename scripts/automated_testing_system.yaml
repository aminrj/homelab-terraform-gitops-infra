# Automated Testing System for LLMs
---
apiVersion: v1
kind: Namespace
metadata:
  name: llm-testing

---
# Test Runner ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-test-runner
  namespace: llm-testing
data:
  test_runner.py: |
    #!/usr/bin/env python3
    import asyncio
    import aiohttp
    import json
    import time
    import logging
    from typing import List, Dict, Any
    import yaml

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    class LLMTester:
        def __init__(self):
            self.gateway_url = "http://llm-gateway-enhanced-service.llm-gateway.svc.cluster.local"
            self.test_suites = self.load_test_suites()
            
        def load_test_suites(self):
            return {
                "security_tests": [
                    {
                        "name": "Brute Force Detection",
                        "input": "ALERT: 50 failed login attempts from 192.168.1.100 in 2 minutes",
                        "task_type": "security_analysis",
                        "expected_keywords": ["HIGH", "brute force", "block", "investigate"],
                        "max_response_time": 10000
                    },
                    {
                        "name": "Malware Detection", 
                        "input": "Suspicious PowerShell execution detected: encoded command downloading from external IP",
                        "task_type": "security_analysis",
                        "expected_keywords": ["HIGH", "malware", "powershell", "isolate"],
                        "max_response_time": 10000
                    }
                ],
                "code_tests": [
                    {
                        "name": "SQL Injection",
                        "input": "def get_user(user_id): return db.execute('SELECT * FROM users WHERE id = ' + user_id)",
                        "task_type": "code_analysis",
                        "expected_keywords": ["SQL injection", "vulnerability", "parameterized"],
                        "max_response_time": 8000
                    },
                    {
                        "name": "Command Injection",
                        "input": "os.system('ping ' + user_input)",
                        "task_type": "code_analysis", 
                        "expected_keywords": ["command injection", "sanitize", "subprocess"],
                        "max_response_time": 8000
                    }
                ],
                "performance_tests": [
                    {
                        "name": "Quick Response",
                        "input": "Hello",
                        "task_type": "quick_test",
                        "max_response_time": 3000,
                        "models": ["phi3:mini", "tinyllama"]
                    },
                    {
                        "name": "Medium Response",
                        "input": "Explain the concept of zero trust security",
                        "task_type": "general",
                        "max_response_time": 15000,
                        "models": ["llama3.2:3b", "mistral:7b"]
                    }
                ]
            }

        async def run_single_test(self, test: Dict[str, Any], model: str = None) -> Dict[str, Any]:
            """Run a single test case"""
            start_time = time.time()
            
            try:
                payload = {
                    "message": test["input"],
                    "task_type": test["task_type"],
                    "model": model,
                    "auto_select_model": model is None
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.gateway_url}/chat",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            response_time = int((time.time() - start_time) * 1000)
                            
                            # Check expectations
                            passed_keywords = []
                            failed_keywords = []
                            
                            if "expected_keywords" in test:
                                for keyword in test["expected_keywords"]:
                                    if keyword.lower() in result["response"].lower():
                                        passed_keywords.append(keyword)
                                    else:
                                        failed_keywords.append(keyword)
                            
                            # Performance check
                            performance_pass = response_time <= test.get("max_response_time", 30000)
                            
                            return {
                                "test_name": test["name"],
                                "model": result.get("model", model),
                                "status": "PASS" if not failed_keywords and performance_pass else "FAIL",
                                "response_time_ms": response_time,
                                "performance_pass": performance_pass,
                                "keywords_found": passed_keywords,
                                "keywords_missing": failed_keywords,
                                "response_length": len(result["response"]),
                                "tokens_per_second": result.get("usage", {}).get("tokens_per_second", 0),
                                "full_response": result["response"][:200] + "..." if len(result["response"]) > 200 else result["response"]
                            }
                        else:
                            return {
                                "test_name": test["name"],
                                "model": model,
                                "status": "ERROR",
                                "error": f"HTTP {response.status}: {await response.text()}"
                            }
                            
            except Exception as e:
                return {
                    "test_name": test["name"],
                    "model": model,
                    "status": "ERROR", 
                    "error": str(e)
                }

        async def run_test_suite(self, suite_name: str, models: List[str] = None) -> Dict[str, Any]:
            """Run a complete test suite"""
            if suite_name not in self.test_suites:
                return {"error": f"Test suite '{suite_name}' not found"}
            
            tests = self.test_suites[suite_name]
            results = []
            
            logger.info(f"Running test suite: {suite_name}")
            
            for test in tests:
                test_models = test.get("models", models or ["phi3:mini"])
                
                for model in test_models:
                    logger.info(f"Running test '{test['name']}' with model '{model}'")
                    result = await self.run_single_test(test, model)
                    results.append(result)
            
            # Calculate summary statistics
            total_tests = len(results)
            passed_tests = len([r for r in results if r["status"] == "PASS"])
            failed_tests = len([r for r in results if r["status"] == "FAIL"])
            error_tests = len([r for r in results if r["status"] == "ERROR"])
            
            avg_response_time = sum([r.get("response_time_ms", 0) for r in results if "response_time_ms" in r]) / len(results) if results else 0
            
            return {
                "suite_name": suite_name,
                "summary": {
                    "total_tests": total_tests,
                    "passed": passed_tests,
                    "failed": failed_tests,
                    "errors": error_tests,
                    "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                    "avg_response_time_ms": round(avg_response_time, 2)
                },
                "results": results,
                "timestamp": time.time()
            }

        async def run_all_tests(self) -> Dict[str, Any]:
            """Run all test suites"""
            all_results = {}
            
            for suite_name in self.test_suites.keys():
                all_results[suite_name] = await self.run_test_suite(suite_name)
            
            return {
                "overall_results": all_results,
                "timestamp": time.time(),
                "total_suites": len(all_results)
            }

    # FastAPI app for test runner
    from fastapi import FastAPI, HTTPException
    from fastapi.responses import HTMLResponse
    from pydantic import BaseModel

    app = FastAPI(title="LLM Test Runner", version="1.0.0")
    tester = LLMTester()

    class TestRequest(BaseModel):
        suite_name: str = None
        models: List[str] = None

    @app.get("/", response_class=HTMLResponse)
    async def dashboard():
        return '''<!DOCTYPE html>
<html>
<head>
    <title>üß™ LLM Test Runner Dashboard</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, sans-serif; margin: 0; background: #f8f9fa; }
        .header { background: linear-gradient(135deg, #28a745, #20c997); color: white; padding: 20px; }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        .card { background: white; border-radius: 12px; padding: 20px; margin: 20px 0; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
        .test-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }
        .test-suite { border: 2px solid #e9ecef; border-radius: 8px; padding: 15px; }
        .test-suite h4 { margin: 0 0 10px 0; color: #495057; }
        button { background: #28a745; color: white; border: none; padding: 10px 20px; border-radius: 6px; cursor: pointer; margin: 5px; }
        button:hover { background: #218838; }
        .status-pass { color: #28a745; font-weight: bold; }
        .status-fail { color: #dc3545; font-weight: bold; }
        .status-error { color: #fd7e14; font-weight: bold; }
        .results { max-height: 400px; overflow-y: auto; background: #f8f9fa; padding: 15px; border-radius: 6px; margin: 10px 0; }
        .metric { display: inline-block; background: #e9ecef; padding: 8px 12px; border-radius: 4px; margin: 5px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>üß™ LLM Test Runner Dashboard</h1>
        <p>Automated testing for your LLM models and workflows</p>
    </div>
    
    <div class="container">
        <div class="card">
            <h2>üöÄ Quick Actions</h2>
            <button onclick="runAllTests()">Run All Tests</button>
            <button onclick="runTestSuite('security_tests')">Security Tests</button>
            <button onclick="runTestSuite('code_tests')">Code Analysis Tests</button>
            <button onclick="runTestSuite('performance_tests')">Performance Tests</button>
            <button onclick="showTestSuites()">View Test Suites</button>
        </div>
        
        <div class="card">
            <h2>üìä Test Results</h2>
            <div id="results">Click a button above to run tests...</div>
        </div>
        
        <div class="card" id="test-suites" style="display: none;">
            <h2>üìã Available Test Suites</h2>
            <div class="test-grid">
                <div class="test-suite">
                    <h4>üîí Security Tests</h4>
                    <p>Tests for security event analysis, threat detection, and risk assessment</p>
                    <ul>
                        <li>Brute Force Detection</li>
                        <li>Malware Analysis</li>
                        <li>Threat Classification</li>
                    </ul>
                </div>
                <div class="test-suite">
                    <h4>üíª Code Analysis Tests</h4>
                    <p>Security code review and vulnerability detection tests</p>
                    <ul>
                        <li>SQL Injection Detection</li>
                        <li>Command Injection</li>
                        <li>XSS Vulnerability</li>
                    </ul>
                </div>
                <div class="test-suite">
                    <h4>‚ö° Performance Tests</h4>
                    <p>Response time and throughput testing across models</p>
                    <ul>
                        <li>Quick Response Tests</li>
                        <li>Medium Complexity</li>
                        <li>Model Comparison</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <script>
        async function runAllTests() {
            showLoading("Running all test suites...");
            try {
                const response = await fetch('/test/all');
                const data = await response.json();
                displayResults(data);
            } catch (error) {
                showError(error.message);
            }
        }
        
        async function runTestSuite(suiteName) {
            showLoading(`Running ${suiteName}...`);
            try {
                const response = await fetch('/test/suite', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ suite_name: suiteName })
                });
                const data = await response.json();
                displayResults(data);
            } catch (error) {
                showError(error.message);
            }
        }
        
        function showTestSuites() {
            const element = document.getElementById('test-suites');
            element.style.display = element.style.display === 'none' ? 'block' : 'none';
        }
        
        function showLoading(message) {
            document.getElementById('results').innerHTML = `<div style="text-align: center; padding: 40px;">${message} ‚è≥</div>`;
        }
        
        function showError(message) {
            document.getElementById('results').innerHTML = `<div style="color: red; padding: 20px;">‚ùå Error: ${message}</div>`;
        }
        
        function displayResults(data) {
            if (data.overall_results) {
                // All tests results
                let html = '<h3>üìä Complete Test Results</h3>';
                for (const [suiteName, suiteResults] of Object.entries(data.overall_results)) {
                    html += formatSuiteResults(suiteResults);
                }
                document.getElementById('results').innerHTML = html;
            } else {
                // Single suite results
                document.getElementById('results').innerHTML = formatSuiteResults(data);
            }
        }
        
        function formatSuiteResults(data) {
            const summary = data.summary;
            const successRate = Math.round(summary.success_rate);
            const statusColor = successRate >= 80 ? '#28a745' : successRate >= 60 ? '#ffc107' : '#dc3545';
            
            let html = `
                <div style="border-left: 4px solid ${statusColor}; padding: 15px; margin: 15px 0;">
                    <h4>${data.suite_name}</h4>
                    <div>
                        <span class="metric">‚úÖ Passed: ${summary.passed}</span>
                        <span class="metric">‚ùå Failed: ${summary.failed}</span>
                        <span class="metric">üö® Errors: ${summary.errors}</span>
                        <span class="metric">üìà Success Rate: ${successRate}%</span>
                        <span class="metric">‚è±Ô∏è Avg Time: ${summary.avg_response_time_ms}ms</span>
                    </div>
                    <div class="results">
            `;
            
            data.results.forEach(result => {
                const statusClass = `status-${result.status.toLowerCase()}`;
                html += `
                    <div style="border-bottom: 1px solid #dee2e6; padding: 10px 0;">
                        <strong>${result.test_name}</strong> 
                        <span class="${statusClass}">[${result.status}]</span><br>
                        <small>Model: ${result.model} | Time: ${result.response_time_ms || 'N/A'}ms</small>
                `;
                
                if (result.keywords_found) {
                    html += `<br><small>‚úÖ Found: ${result.keywords_found.join(', ')}</small>`;
                }
                if (result.keywords_missing && result.keywords_missing.length > 0) {
                    html += `<br><small>‚ùå Missing: ${result.keywords_missing.join(', ')}</small>`;
                }
                if (result.error) {
                    html += `<br><small style="color: red;">Error: ${result.error}</small>`;
                }
                
                html += '</div>';
            });
            
            html += '</div></div>';
            return html;
        }
    </script>
</body>
</html>'''

    @app.post("/test/suite")
    async def run_test_suite(request: TestRequest):
        if not request.suite_name:
            raise HTTPException(status_code=400, detail="suite_name is required")
        
        results = await tester.run_test_suite(request.suite_name, request.models)
        return results

    @app.get("/test/all")
    async def run_all_tests():
        results = await tester.run_all_tests()
        return results

    @app.get("/test/suites")
    async def list_test_suites():
        return {"suites": list(tester.test_suites.keys())}

    @app.get("/health")
    async def health():
        return {"status": "healthy", "timestamp": time.time()}

    if __name__ == "__main__":
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8080)

---
# Enhanced n8n ConfigMap with LLM Testing Nodes
apiVersion: v1
kind: ConfigMap
metadata:
  name: n8n-enhanced-llm
  namespace: n8n
data:
  LLMTestRunner.node.js: |
    const { NodeOperationError } = require('n8n-core');

    class LLMTestRunner {
      constructor() {
        this.description = {
          displayName: 'LLM Test Runner',
          name: 'llmTestRunner',
          group: ['transform'],
          version: 1,
          description: 'Run automated tests on LLM models',
          defaults: { name: 'LLM Test Runner' },
          inputs: ['main'],
          outputs: ['main'],
          properties: [
            {
              displayName: 'Test Action',
              name: 'testAction',
              type: 'options',
              options: [
                { name: 'Run Test Suite', value: 'run_suite' },
                { name: 'Run All Tests', value: 'run_all' },
                { name: 'Single Test', value: 'single_test' }
              ],
              default: 'run_suite'
            },
            {
              displayName: 'Test Suite',
              name: 'testSuite',
              type: 'options',
              options: [
                { name: 'Security Tests', value: 'security_tests' },
                { name: 'Code Analysis Tests', value: 'code_tests' },
                { name: 'Performance Tests', value: 'performance_tests' }
              ],
              default: 'security_tests',
              displayOptions: {
                show: { testAction: ['run_suite'] }
              }
            },
            {
              displayName: 'Models to Test',
              name: 'models',
              type: 'string',
              default: 'phi3:mini,llama3.2:3b',
              description: 'Comma-separated list of models to test'
            }
          ]
        };
      }

      async execute() {
        const items = this.getInputData();
        const returnData = [];

        for (let i = 0; i < items.length; i++) {
          try {
            const testAction = this.getNodeParameter('testAction', i);
            const testSuite = this.getNodeParameter('testSuite', i);
            const models = this.getNodeParameter('models', i).split(',').map(m => m.trim());
            
            const testRunnerUrl = process.env.LLM_TEST_RUNNER_URL || 'http://llm-test-runner-service.llm-testing.svc.cluster.local';
            
            let url, payload;
            
            if (testAction === 'run_all') {
              url = `${testRunnerUrl}/test/all`;
              payload = null;
            } else if (testAction === 'run_suite') {
              url = `${testRunnerUrl}/test/suite`;
              payload = { suite_name: testSuite, models };
            }
            
            const response = await this.helpers.request({
              method: payload ? 'POST' : 'GET',
              url,
              json: payload
            });

            returnData.push({
              json: {
                ...items[i].json,
                test_results: response,
                test_action: testAction,
                models_tested: models,
                timestamp: new Date().toISOString()
              }
            });
          } catch (error) {
            if (this.continueOnFail()) {
              returnData.push({
                json: { ...items[i].json, error: error.message }
              });
            } else {
              throw new NodeOperationError(this.getNode(), error);
            }
          }
        }

        return [returnData];
      }
    }

    module.exports = { LLMTestRunner };

  LLMModelManager.node.js: |
    const { NodeOperationError } = require('n8n-core');

    class LLMModelManager {
      constructor() {
        this.description = {
          displayName: 'LLM Model Manager',
          name: 'llmModelManager',
          group: ['transform'],
          version: 1,
          description: 'Manage LLM models and deployments',
          defaults: { name: 'LLM Model Manager' },
          inputs: ['main'],
          outputs: ['main'],
          properties: [
            {
              displayName: 'Action',
              name: 'action',
              type: 'options',
              options: [
                { name: 'List Models', value: 'list_models' },
                { name: 'Check Model Health', value: 'check_health' },
                { name: 'Get Model Stats', value: 'get_stats' },
                { name: 'Compare Models', value: 'compare_models' }
              ],
              default: 'list_models'
            },
            {
              displayName: 'Test Message',
              name: 'testMessage',
              type: 'string',
              default: 'Hello, how are you?',
              description: 'Message to test models with',
              displayOptions: {
                show: { action: ['compare_models'] }
              }
            }
          ]
        };
      }

      async execute() {
        const items = this.getInputData();
        const returnData = [];

        for (let i = 0; i < items.length; i++) {
          try {
            const action = this.getNodeParameter('action', i);
            const gatewayUrl = process.env.LLM_GATEWAY_URL || 'http://llm-gateway-enhanced-service.llm-gateway.svc.cluster.local';
            
            let result;

            if (action === 'list_models') {
              result = await this.helpers.request({
                method: 'GET',
                url: `${gatewayUrl}/models`
              });
            } else if (action === 'check_health') {
              result = await this.helpers.request({
                method: 'GET', 
                url: `${gatewayUrl}/health`
              });
            } else if (action === 'compare_models') {
              const testMessage = this.getNodeParameter('testMessage', i);
              const models = ['phi3:mini', 'llama3.2:3b', 'mistral:7b'];
              
              const comparisons = [];
              for (const model of models) {
                try {
                  const start = Date.now();
                  const response = await this.helpers.request({
                    method: 'POST',
                    url: `${gatewayUrl}/chat`,
                    json: {
                      message: testMessage,
                      model: model,
                      auto_select_model: false
                    }
                  });
                  
                  comparisons.push({
                    model,
                    response_time_ms: Date.now() - start,
                    tokens_per_second: response.usage.tokens_per_second || 0,
                    response_length: response.response.length,
                    status: 'success'
                  });
                } catch (error) {
                  comparisons.push({
                    model,
                    status: 'error',
                    error: error.message
                  });
                }
              }
              
              result = { model_comparison: comparisons };
            }

            returnData.push({
              json: {
                ...items[i].json,
                llm_management_result: result,
                action: action,
                timestamp: new Date().toISOString()
              }
            });
          } catch (error) {
            if (this.continueOnFail()) {
              returnData.push({
                json: { ...items[i].json, error: error.message }
              });
            } else {
              throw new NodeOperationError(this.getNode(), error);
            }
          }
        }

        return [returnData];
      }
    }

    module.exports = { LLMModelManager };

---
# Test Runner Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-test-runner
  namespace: llm-testing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-test-runner
  template:
    metadata:
      labels:
        app: llm-test-runner
    spec:
      containers:
      - name: test-runner
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install fastapi uvicorn aiohttp pydantic pyyaml
          mkdir -p /app
          cp /config/test_runner.py /app/test_runner.py
          cd /app && uvicorn test_runner:app --host 0.0.0.0 --port 8080
        ports:
        - containerPort: 8080
        env:
        - name: LLM_GATEWAY_URL
          value: "http://llm-gateway-enhanced-service.llm-gateway.svc.cluster.local"
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        volumeMounts:
        - name: test-config
          mountPath: /config
      volumes:
      - name: test-config
        configMap:
          name: llm-test-runner

---
apiVersion: v1
kind: Service
metadata:
  name: llm-test-runner-service
  namespace: llm-testing
spec:
  selector:
    app: llm-test-runner
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP

---
# Ingress for Test Runner
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-test-runner-ingress
  namespace: llm-testing
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: llm-tests.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-test-runner-service
            port:
              number: 80
            